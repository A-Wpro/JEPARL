Episode: 1/1000, Score: -1, Epsilon: 1.0
Episode: 2/1000, Score: 41, Epsilon: 1.0
Episode: 3/1000, Score: 82, Epsilon: 1.0
Episode: 4/1000, Score: 41, Epsilon: 1.0
Episode: 5/1000, Score: 26, Epsilon: 1.0
Episode: 6/1000, Score: 33, Epsilon: 1.0
Episode: 7/1000, Score: 260, Epsilon: 0.99
Episode: 8/1000, Score: 39, Epsilon: 0.99
Episode: 9/1000, Score: 35, Epsilon: 0.99
Episode: 10/1000, Score: 5, Epsilon: 0.99
Episode: 11/1000, Score: 16, Epsilon: 0.99
Episode: 12/1000, Score: 51, Epsilon: 0.99
Episode: 13/1000, Score: 98, Epsilon: 0.99
Episode: 14/1000, Score: 115, Epsilon: 0.99
Episode: 15/1000, Score: 174, Epsilon: 0.99
Episode: 16/1000, Score: -4, Epsilon: 0.99
Episode: 17/1000, Score: 126, Epsilon: 0.99
Episode: 18/1000, Score: 20, Epsilon: 0.99
Episode: 19/1000, Score: 21, Epsilon: 0.99
Episode: 20/1000, Score: 3, Epsilon: 0.99
Episode: 21/1000, Score: 16, Epsilon: 0.99
Episode: 22/1000, Score: 94, Epsilon: 0.99
Episode: 23/1000, Score: 91, Epsilon: 0.98
Episode: 24/1000, Score: 9, Epsilon: 0.98
Episode: 25/1000, Score: 45, Epsilon: 0.98
Episode: 26/1000, Score: 5, Epsilon: 0.98
Episode: 27/1000, Score: 178, Epsilon: 0.98
Traceback (most recent call last):
  File "c:\Python\JEPARL\train_model.py", line 47, in <module>
    train_model()
  File "c:\Python\JEPARL\train_model.py", line 34, in train_model
    agent.replay(batch_size)
  File "c:\Python\JEPARL\dqn_agent.py", line 69, in replay
    self.scaler.step(self.optimizer)
  File "C:\Users\adamw\.conda\envs\LLM\Lib\site-packages\torch\cuda\amp\grad_scaler.py", line 452, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\adamw\.conda\envs\LLM\Lib\site-packages\torch\cuda\amp\grad_scaler.py", line 350, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\adamw\.conda\envs\LLM\Lib\site-packages\torch\optim\lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\adamw\.conda\envs\LLM\Lib\site-packages\torch\optim\optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\adamw\.conda\envs\LLM\Lib\site-packages\torch\optim\optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\adamw\.conda\envs\LLM\Lib\site-packages\torch\optim\adam.py", line 166, in step
    adam(
  File "C:\Users\adamw\.conda\envs\LLM\Lib\site-packages\torch\optim\adam.py", line 316, in adam
    func(params,
  File "C:\Users\adamw\.conda\envs\LLM\Lib\site-packages\torch\optim\adam.py", line 582, in _multi_tensor_adam
    torch._foreach_add_(exp_avg_sq_sqrt, eps)
KeyboardInterrupt
